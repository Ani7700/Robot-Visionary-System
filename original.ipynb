{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the EE4309 2024/25 S1 Lab Project: Object Detection (Inference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load an image and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "image = read_image(path = './demo.jpg')\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build a ResNet backbone\n",
    "This is the first step to implement a Faster R-CNN! We will provide some useful APIs for convenience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Useful APIs: conv1x1, conv3x3, Bottleneck, _make_block\n",
    "Kindly use these functions and classes in the subsequent sections. You may need to complete some key parts on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion: int = 4\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        if stride > 1 or inplanes != planes * self.expansion:\n",
    "            self.downsample = nn.Sequential(\n",
    "                conv1x1(inplanes, planes * self.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * self.expansion),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (2) Please write the pipeline in the bottleneck. It should be \n",
    "\n",
    "        # -> conv1 -> bn1 -> relu -> conv2 -> bn2 -> relu -> conv3 -> bn3 -> (add) -> relu ->\n",
    "        # |                                                                    |\n",
    "        # |--------------------(may downsample, or identity)-------------------|\n",
    "\n",
    "        ### --- Please write you code here --- ###\n",
    "        \n",
    "        # Save the input tensor\n",
    "        identity = x\n",
    "\n",
    "        # The pipeline\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        # Apply downsample if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # Add the identity\n",
    "        out += identity\n",
    "\n",
    "        # Final ReLU activation\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "        ### --- Please write you code here --- ###\n",
    "    \n",
    "def _make_block(\n",
    "    inplanes: int,\n",
    "    planes: int,\n",
    "    blocks: int,\n",
    "    stride: int = 1,\n",
    ") -> nn.Module:\n",
    "    block = [Bottleneck(inplanes, planes, stride)]\n",
    "    outplanes = planes * Bottleneck.expansion\n",
    "    for _ in range(1, blocks):\n",
    "        block.append(Bottleneck(outplanes, planes))\n",
    "    return nn.Sequential(*block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Use provided APIs to build a ResNet\n",
    "Think carefully, what should be the missing parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: list[int] = [3, 4, 6, 3],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        base_width: int = 64,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.inplanes = base_width\n",
    "\n",
    "        # (3) Please write some layers before residual blocks: conv2d (ksize = 7, stride=2, padding=3, bias=False) -> bn -> relu -> maxpool(ksize = 3, stride=2, padding=1)\n",
    "        ### --- Please write you code here --- ###\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        ### --- Please write you code here --- ###\n",
    "\n",
    "        expansion = Bottleneck.expansion\n",
    "        self.layer1 = _make_block(self.inplanes, 64, layers[0])\n",
    "        self.layer2 = _make_block(64 * expansion, 128, layers[1], stride=2)\n",
    "        self.layer3 = _make_block(128 * expansion, 256, layers[2], stride=2)\n",
    "        self.layer4 = _make_block(256 * expansion, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * Bottleneck.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "resnet = ResNet()\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Load the Pre-trained ResNet Checkpoint for Image Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) load checkpoint resnet50.pth to previous resnet\n",
    "### --- Please write you code here --- ###\n",
    "\n",
    "import torch\n",
    "\n",
    "# (4) Load checkpoint resnet50.pth to previous resnet\n",
    "# Load the state_dict from the checkpoint\n",
    "checkpoint = torch.load('resnet50.pth')\n",
    "\n",
    "# Load the state_dict into the resnet model\n",
    "resnet.load_state_dict(checkpoint)\n",
    "\n",
    "### --- Please write you code here --- ###\n",
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Intermediate Check\n",
    "Execute the code provided below and observe if the results from the image classification appear reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models._meta import _IMAGENET_CATEGORIES\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "\n",
    "x = ImageClassification(crop_size=224, resize_size=232)(image).unsqueeze(0)\n",
    "with torch.inference_mode():\n",
    "    scores = resnet(x).softmax(dim=-1).squeeze_()\n",
    "sorted_ids = scores.argsort(descending=True).tolist()\n",
    "for class_id in sorted_ids:\n",
    "    score = scores[class_id].item()\n",
    "    category_name = _IMAGENET_CATEGORIES[class_id]\n",
    "    print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing the Image for Object Detection\n",
    "Until now, our focus has been on image classification. However, before transitioning to object detection, it's essential to preprocess the image appropriately. Typically, object detection requires high-resolution images (e.g., 800x1333), in contrast to the smaller resolutions used for image classification (e.g., 224x224)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Convert Image Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) torch.uint8 -> torch.float, and ensure 0~1 range\n",
    "\n",
    "### --- Please write you code here --- ###\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from PIL import Image\n",
    "\n",
    "def convert_image_dtype(image, dtype=torch.float32):\n",
    "    # Ensure the input image is converted to a tensor if it's a PIL Image\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = to_tensor(image)\n",
    "\n",
    "    # Ensure the input image is a tensor\n",
    "    if not isinstance(image, torch.Tensor):\n",
    "        raise TypeError(f\"Expected input to be a torch.Tensor, got {type(image)} instead.\")\n",
    "\n",
    "    # Scale and convert to the desired data type\n",
    "    return image.float() / 255.0 if dtype == torch.float32 else image.to(dtype)\n",
    "\n",
    "### --- Please write you code here --- ###\n",
    "\n",
    "image_float = convert_image_dtype(image)\n",
    "print(image_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Resize and Normalize the Image\n",
    "When resizing images for object detection, it's essential to maintain the original aspect ratio. Given that images can come in various shapes, it's also important to note their original dimensions before resizing them to a consistent scale. To simplify this process, you don't need to write any code. Instead, just run the provided code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform, ImageList\n",
    "\n",
    "min_size = 800\n",
    "max_size = 1333\n",
    "image_mean = [0.485, 0.456, 0.406]\n",
    "image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)\n",
    "transform.eval()\n",
    "images, _ = transform([image_float], targets=None)\n",
    "print(images, images.tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image has been scaled to a minimum edge size of 800. The transformed 'x' is an instance of 'ImageList'. You don't need to be familiar with its internal details; just understand that it represents a structure that holds a list of images (which may vary in size) as a single tensor. x.tensors can access the image's transformed pytorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Constructing a Feature Extractor using ResNet\n",
    "It's essential to note that ResNet is primarily a classification network, not a detection network. However, in the Faster R-CNN framework, ResNet serves as a feature extractor. Upon this foundation, we can further construct the Region Proposal Network (RPN) and Region of Interest (RoI) sub-network. This section will guide you through this process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 The Handy API: IntermediateLayerGetter\n",
    "Suppose we want to retrieve features after the 'layer4' but before 'avgpool'. How can we achieve this? We can utilize an API from torchvision called IntermediateLayerGetter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "feature_extractor = IntermediateLayerGetter(resnet, {'layer4': 'feat4'})\n",
    "features = feature_extractor(images.tensors)\n",
    "print(features['feat4'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we successfully extracted the feature map, which has 2048 channels, 25x38 spatial size. (800/32, 1216/32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Extract multiple layer features from ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Please use IntermediateLayerGetter to get multiple layer features. the output should satisfy: \n",
    "# '0': layer1 feature, '1': layer2 feature, '2': layer3 feature, '3': layer4 feature\n",
    "\n",
    "returned_layers = [1,2,3,4]\n",
    "### --- Please write you code here --- ###\n",
    "\n",
    "return_layers = {'layer1': '0', 'layer2':'1', 'layer3':'2', 'layer4':'3'}\n",
    "feature_extractor = IntermediateLayerGetter(resnet, return_layers = return_layers)\n",
    "x = torch.rand(1,3,224,224)\n",
    "features = feature_extractor(x)\n",
    "\n",
    "### --- Please write you code here --- ###\n",
    "\n",
    "print(features['0'].shape, features['1'].shape, features['2'].shape, features['3'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Constructing a Feature Pyramid Network (FPN) on the Feature Map\n",
    "For an introduction to the Feature Pyramid Network, visit this link: https://paperswithcode.com/method/fpn. Note: As this topic isn't covered in our course, you can proceed by directly executing the provided code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelMaxPool\n",
    "\n",
    "class BackboneWithFPN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        return_layers: dict[str, str],\n",
    "        in_channels_list: list[int],\n",
    "        out_channels: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=in_channels_list,\n",
    "            out_channels=out_channels,\n",
    "            extra_blocks=LastLevelMaxPool(),\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "        )\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        x = self.body(x)\n",
    "        x = self.fpn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Load Pre-trained Checkpoint of the feature extractor and do forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_layers = [1, 2, 3, 4]\n",
    "return_layers = {'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}\n",
    "in_channels_stage2 = resnet.fc.in_features // 8\n",
    "# in_channels_list = [int(in_channels_stage2 * 2 ** i) for i in range(len(return_layers))]\n",
    "in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n",
    "out_channels = 256\n",
    "feature_extractor = BackboneWithFPN(resnet, return_layers, in_channels_list, out_channels)\n",
    "feature_extractor.load_state_dict(torch.load('./feature_extractor.pth'))\n",
    "feature_extractor.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = feature_extractor(images.tensors)\n",
    "print([(k, v.shape) for k, v in features.items()])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build Anchor Boxes and Region Proposal Network (RPN) on the Feature Map\n",
    "In this section, we will construct the Anchor Boxes and the Region Proposal Network (RPN) on the feature map. This represents the 'region proposal' step, which is the 1st stage in Faster R-CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Build Anchor Boxes Generator\n",
    "An anchor box is essentially a predefined bounding box shape (i.e., width and height) at a specific location in the image. For a given feature map location, multiple anchor boxes (of different shapes/sizes) can be associated. To detect objects of various sizes and aspect ratios, anchor boxes are defined at multiple scales and aspect ratios. For example, at a single feature map location, you might have three scales and three aspect ratios, yielding nine anchor boxes. \n",
    "\n",
    "Writing code to generate anchor boxes can be tedious. Please execute the following code and examine the resulting anchor boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "rpn_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "anchors = rpn_anchor_generator(images, list(features.values()))\n",
    "print(anchors[0], anchors[0].shape) # we only have 1 image, so index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) Can you visualize timport random\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import resize, to_tensor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from PIL import Image\n",
    "\n",
    "# Function to generate random anchor boxes\n",
    "def generate_random_anchors(num_anchors, image_width, image_height):\n",
    "    anchors = []\n",
    "    for _ in range(num_anchors):\n",
    "        # Randomly select width, height and top-left corner of the anchor\n",
    "        width = random.randint(50, 150)   # Random width between 50 and 150 pixels\n",
    "        height = random.randint(50, 150)  # Random height between 50 and 150 pixels\n",
    "        x = random.randint(0, image_width - width)\n",
    "        y = random.randint(0, image_height - height)\n",
    "        anchors.append((x, y, x + width, y + height))\n",
    "    return anchors\n",
    "\n",
    "# Visualize random anchors on the image using draw_bounding_boxes\n",
    "def visualize_anchors(image, anchors):\n",
    "    # Convert the image tensor to [B, C, H, W] format\n",
    "    image_with_bboxes = draw_bounding_boxes(image, boxes=torch.tensor(anchors), colors=\"red\", width=2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_with_bboxes.permute(1, 2, 0).cpu().numpy())  # Convert CHW to HWC for plotting\n",
    "    plt.axis('off')\n",
    "    plt.title('Random Anchors on Image')\n",
    "    plt.show()\n",
    "\n",
    "# Resize the image for visualization\n",
    "image_just_for_vis = resize(image, (800, 1216))\n",
    "\n",
    "# Generate 1000 random anchors\n",
    "num_random_anchors = 1000\n",
    "image_width = image_just_for_vis.shape[2]\n",
    "image_height = image_just_for_vis.shape[1]\n",
    "random_anchors = generate_random_anchors(num_random_anchors, image_width, image_height)\n",
    "\n",
    "# Visualize the anchors\n",
    "visualize_anchors(image_just_for_vis, random_anchors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Build RPN Head Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) Please complete RPNHead __init__ and forward function.\n",
    "\n",
    "from torchvision.ops import Conv2dNormActivation\n",
    "class RPNHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds a simple RPN Head with classification and regression heads\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of channels of the input feature\n",
    "        num_anchors (int): number of anchors to be predicted\n",
    "        conv_depth (int, optional): number of convolutions\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, num_anchors: int, conv_depth=1) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        convs = []\n",
    "        for _ in range(conv_depth):\n",
    "            convs.append(Conv2dNormActivation(in_channels, in_channels, kernel_size=3, norm_layer=None))\n",
    "        self.conv = nn.Sequential(*convs)\n",
    "        ### --- Please write you code here --- ###\n",
    "        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=1)\n",
    "        self.bbox_pred = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1)\n",
    "        ### --- Please write you code here --- ###\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                torch.nn.init.normal_(layer.weight, std=0.01) \n",
    "                if layer.bias is not None:\n",
    "                    torch.nn.init.constant_(layer.bias, 0) \n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            for type in [\"weight\", \"bias\"]:\n",
    "                old_key = f\"{prefix}conv.{type}\"\n",
    "                new_key = f\"{prefix}conv.0.0.{type}\"\n",
    "                if old_key in state_dict:\n",
    "                    state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: list[torch.Tensor]) -> tuple[list[torch.Tensor], list[torch.Tensor]]:\n",
    "        cls_logits = []\n",
    "        bbox_reg = []\n",
    "        for feature in x: # feature from multi layers\n",
    "            ### --- Please write you code here --- ###\n",
    "            t = self.conv(feature)\n",
    "            cls_logits.append(self.cls_logits(t))\n",
    "            bbox_reg.append(self.bbox_pred(t))\n",
    "            ### --- Please write you code here --- ###\n",
    "        return cls_logits, bbox_reg\n",
    "\n",
    "rpn_head = RPNHead(feature_extractor.out_channels, rpn_anchor_generator.num_anchors_per_location()[0], conv_depth=2)\n",
    "print(rpn_head)\n",
    "rpn_head.load_state_dict(torch.load('rpn_head.pth'))\n",
    "objectness, pred_bbox_deltas = rpn_head(list(features.values()))\n",
    "print('objectness shape', [o.shape for o in objectness])\n",
    "print('pred_bbox_deltas shape', [b.shape for b in pred_bbox_deltas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Build Region Proposal Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class BoxCoder:\n",
    "    \"\"\"\n",
    "    This class encodes and decodes a set of bounding boxes into\n",
    "    the representation used for training the regressors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, weights: tuple[float, float, float, float], bbox_xform_clip: float = math.log(1000.0 / 16)\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (4-element tuple)\n",
    "            bbox_xform_clip (float)\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.bbox_xform_clip = bbox_xform_clip\n",
    "\n",
    "    def decode(self, rel_codes: torch.Tensor, boxes: list[torch.Tensor]) -> torch.Tensor:\n",
    "        torch._assert(\n",
    "            isinstance(boxes, (list, tuple)),\n",
    "            \"This function expects boxes of type list or tuple.\",\n",
    "        )\n",
    "        torch._assert(\n",
    "            isinstance(rel_codes, torch.Tensor),\n",
    "            \"This function expects rel_codes of type torch.Tensor.\",\n",
    "        )\n",
    "        \n",
    "        boxes_per_image = [b.size(0) for b in boxes]\n",
    "        concat_boxes = torch.cat(boxes, dim=0)\n",
    "        box_sum = sum(boxes_per_image)\n",
    "        \n",
    "        if box_sum > 0:\n",
    "            rel_codes = rel_codes.reshape(box_sum, -1)\n",
    "        \n",
    "        concat_boxes = concat_boxes.to(rel_codes.dtype)\n",
    "\n",
    "        widths = concat_boxes[:, 2] - concat_boxes[:, 0]\n",
    "        heights = concat_boxes[:, 3] - concat_boxes[:, 1]\n",
    "        ctr_x = concat_boxes[:, 0] + 0.5 * widths\n",
    "        ctr_y = concat_boxes[:, 1] + 0.5 * heights\n",
    "\n",
    "        wx, wy, ww, wh = self.weights\n",
    "        dx = rel_codes[:, 0::4] / wx\n",
    "        dy = rel_codes[:, 1::4] / wy\n",
    "        dw = rel_codes[:, 2::4] / ww\n",
    "        dh = rel_codes[:, 3::4] / wh\n",
    "\n",
    "        # Prevent sending too large values into torch.exp()\n",
    "        dw = torch.clamp(dw, max=self.bbox_xform_clip)\n",
    "        dh = torch.clamp(dh, max=self.bbox_xform_clip)\n",
    "\n",
    "        # Calculate predicted boxes\n",
    "        pred_boxes = torch.zeros_like(rel_codes)\n",
    "\n",
    "        pred_boxes[:, 0::4] = ctr_x + dx * widths  # x1\n",
    "        pred_boxes[:, 1::4] = ctr_y + dy * heights  # y1\n",
    "        pred_boxes[:, 2::4] = pred_boxes[:, 0::4] + torch.exp(dw) * widths  # x2\n",
    "        pred_boxes[:, 3::4] = pred_boxes[:, 1::4] + torch.exp(dh) * heights  # y2\n",
    "\n",
    "        if box_sum > 0:\n",
    "            pred_boxes = pred_boxes.reshape(box_sum, -1, 4)\n",
    "        return pred_boxes\n",
    "\n",
    "# Example usage\n",
    "# Assuming `anchors` and `pred_bbox_deltas` are defined\n",
    "num_images = len(anchors)\n",
    "num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
    "num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
    "\n",
    "from torchvision.models.detection.rpn import concat_box_prediction_layers\n",
    "objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
    "\n",
    "# Decode the predicted bounding boxes\n",
    "proposals = BoxCoder((1, 1, 1, 1)).decode(pred_bbox_deltas.detach(), anchors)\n",
    "\n",
    "print(\"Anchors:\", anchors)\n",
    "print(\"Proposals:\", proposals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the original anchors have been adjusted. After anchor boxes are justed by the RPNHead bounding-box regression outputs, we should filter most anchor boxes by its class scores (only maintain top 1000) and then perform NMS algorithm to them. If implementing from scratch, it will take a lot of time. Please directly run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_pre_nms_top_n_test = 1000\n",
    "rpn_post_nms_top_n_test = 1000\n",
    "small_box_min_size = 1e-3\n",
    "rpn_nms_thresh = 0.7\n",
    "rpn_score_thresh = 0.0\n",
    "\n",
    "from torchvision import ops as box_ops\n",
    "from torchvision.models.detection._utils import _topk_min\n",
    "\n",
    "def _get_top_n_idx(objectness: torch.Tensor, num_anchors_per_level: list[int]) -> torch.Tensor:\n",
    "    r = []\n",
    "    offset = 0\n",
    "    for ob in objectness.split(num_anchors_per_level, 1):\n",
    "        num_anchors = ob.shape[1]\n",
    "        pre_nms_top_n = _topk_min(ob, rpn_pre_nms_top_n_test, 1)\n",
    "        _, top_n_idx = ob.topk(pre_nms_top_n, dim=1)\n",
    "        r.append(top_n_idx + offset)\n",
    "        offset += num_anchors\n",
    "    return torch.cat(r, dim=1)\n",
    "\n",
    "def filter_proposals(\n",
    "    proposals: torch.Tensor,\n",
    "    objectness: torch.Tensor,\n",
    "    image_shapes: list[tuple[int, int]],\n",
    "    num_anchors_per_level: list[int],\n",
    ") -> tuple[list[torch.Tensor], tuple[torch.Tensor]]:\n",
    "\n",
    "    num_images = proposals.shape[0]\n",
    "    device = proposals.device\n",
    "    objectness = objectness.detach()\n",
    "    objectness = objectness.reshape(num_images, -1)\n",
    "\n",
    "    levels = [\n",
    "        torch.full((n,), idx, dtype=torch.int64, device=device) for idx, n in enumerate(num_anchors_per_level)\n",
    "    ]\n",
    "    levels = torch.cat(levels, 0)\n",
    "    levels = levels.reshape(1, -1).expand_as(objectness)\n",
    "\n",
    "    # select top_n boxes independently per level before applying nms\n",
    "    top_n_idx = _get_top_n_idx(objectness, num_anchors_per_level)\n",
    "\n",
    "    image_range = torch.arange(num_images, device=device)\n",
    "    batch_idx = image_range[:, None]\n",
    "\n",
    "    objectness = objectness[batch_idx, top_n_idx]\n",
    "    levels = levels[batch_idx, top_n_idx]\n",
    "    proposals = proposals[batch_idx, top_n_idx]\n",
    "\n",
    "    objectness_prob = torch.sigmoid(objectness)\n",
    "\n",
    "    final_boxes = []\n",
    "    final_scores = []\n",
    "    for boxes, scores, lvl, img_shape in zip(proposals, objectness_prob, levels, image_shapes):\n",
    "        boxes = box_ops.clip_boxes_to_image(boxes, img_shape)\n",
    "\n",
    "        # remove small boxes\n",
    "        keep = box_ops.remove_small_boxes(boxes, small_box_min_size)\n",
    "        boxes, scores, lvl = boxes[keep], scores[keep], lvl[keep]\n",
    "\n",
    "        # remove low scoring boxes\n",
    "        # use >= for Backwards compatibility\n",
    "        keep = torch.where(scores >= rpn_score_thresh)[0]\n",
    "        boxes, scores, lvl = boxes[keep], scores[keep], lvl[keep]\n",
    "\n",
    "        # non-maximum suppression, independently done per level\n",
    "        keep = box_ops.batched_nms(boxes, scores, lvl, rpn_nms_thresh)\n",
    "\n",
    "        # keep only topk scoring predictions\n",
    "        keep = keep[:rpn_post_nms_top_n_test]\n",
    "        boxes, scores = boxes[keep], scores[keep]\n",
    "\n",
    "        final_boxes.append(boxes)\n",
    "        final_scores.append(scores)\n",
    "    return final_boxes, final_scores\n",
    "\n",
    "proposals = proposals.view(num_images, -1, 4)\n",
    "proposals, scores = filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
    "print('proposals shape', [p.shape for p in proposals])\n",
    "print('scores shape', [s.shape for s in scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Visualize the proposed region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (10) This is an intermediate check. Please reuse your visualization code to visualize the boxes, and put it on the image_just_for_vis. The region proposal should roughly locates objects\n",
    "\n",
    "from torchvision.transforms.functional import resize\n",
    "image_just_for_vis = resize(image, (800, 1216))\n",
    "\n",
    "### --- Please write you code here --- ###\n",
    "\n",
    "### --- Please write you code here --- ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. RoI Align and Subnetwork\n",
    "Congratulations! You've successfully generated region proposals. The subsequent step involves using the RoI Align and RoI-subnetwork to refine these proposal boxes and classify the object's category. This process is much simpler than region proposal network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Multi-scale RoI Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "box_roi_pool = MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=2)\n",
    "\n",
    "# NOTE: RoI align does not have learnable parameters\n",
    "\n",
    "box_features = box_roi_pool(features, proposals, images.image_sizes)\n",
    "print(box_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Box Head and Box Predictor\n",
    "Box head is defined as the network before RoI Network outputs class and bounding-box regression results --- it is used to enhance the feature. Box predictor has two linear layers, one for bounding-box regression and one for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNConvFCHead\n",
    "box_head = FastRCNNConvFCHead(\n",
    "    (feature_extractor.out_channels, 7, 7), [256, 256, 256, 256], [1024], norm_layer=nn.BatchNorm2d\n",
    ")\n",
    "box_head.load_state_dict(torch.load('box_head.pth'))\n",
    "box_head.eval()\n",
    "\n",
    "box_features = box_head(box_features)\n",
    "print(box_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (11) Please implement box_predictor\n",
    "\n",
    "class BoxPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \n",
    "    Inputs: \n",
    "        x (torch.Tensor): with shape num_rois x channels \n",
    "\n",
    "    Returns:\n",
    "        scores (torch.Tensor) \n",
    "        bbox_deltas (torch.Tensor)\n",
    "\n",
    "    Layers:\n",
    "        self.cls_score: nn.Linear \n",
    "        self.bbox_pred: nn.Linear\n",
    "    \"\"\"\n",
    "    ### --- Please write you code here --- ###\n",
    "    \n",
    "    ### --- Please write you code here --- ###\n",
    "\n",
    "representation_size = 1024\n",
    "num_classes = 91\n",
    "box_predictor = BoxPredictor(representation_size, num_classes)\n",
    "box_predictor.load_state_dict(torch.load('box_predictor.pth'))\n",
    "box_predictor.eval()\n",
    "\n",
    "class_logits, box_regression = box_predictor(box_features)\n",
    "print(class_logits.shape, box_regression.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Get Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (12) This is a final check. If success, you will see many objects will be detected in the image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.models._meta import _COCO_CATEGORIES\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "box_score_thresh = 0.05\n",
    "box_nms_thresh = 0.5\n",
    "box_detections_per_img = 100\n",
    "\n",
    "def postprocess_detections(\n",
    "    class_logits,  \n",
    "    box_regression,  \n",
    "    proposals,  \n",
    "    image_shapes, \n",
    "):\n",
    "    device = class_logits.device\n",
    "    num_classes = class_logits.shape[-1]\n",
    "\n",
    "    boxes_per_image = [boxes_in_image.shape[0] for boxes_in_image in proposals]\n",
    "    pred_boxes = BoxCoder((10.0, 10.0, 5.0, 5.0)).decode(box_regression, proposals)\n",
    "\n",
    "    pred_scores = torch.softmax(class_logits, -1)\n",
    "\n",
    "    pred_boxes_list = pred_boxes.split(boxes_per_image, 0)\n",
    "    pred_scores_list = pred_scores.split(boxes_per_image, 0)\n",
    "\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    for boxes, scores, image_shape in zip(pred_boxes_list, pred_scores_list, image_shapes):\n",
    "        boxes = box_ops.clip_boxes_to_image(boxes, image_shape)\n",
    "\n",
    "        # create labels for each prediction\n",
    "        labels = torch.arange(num_classes, device=device)\n",
    "        labels = labels.view(1, -1).expand_as(scores)\n",
    "\n",
    "        # remove predictions with the background label\n",
    "        boxes = boxes[:, 1:]\n",
    "        scores = scores[:, 1:]\n",
    "        labels = labels[:, 1:]\n",
    "\n",
    "        # batch everything, by making every class prediction be a separate instance\n",
    "        boxes = boxes.reshape(-1, 4)\n",
    "        scores = scores.reshape(-1)\n",
    "        labels = labels.reshape(-1)\n",
    "\n",
    "        # remove low scoring boxes\n",
    "        inds = torch.where(scores > box_score_thresh)[0]\n",
    "        boxes, scores, labels = boxes[inds], scores[inds], labels[inds]\n",
    "\n",
    "        # remove empty boxes\n",
    "        keep = box_ops.remove_small_boxes(boxes, min_size=1e-2)\n",
    "        boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
    "\n",
    "        # non-maximum suppression, independently done per class\n",
    "        keep = box_ops.batched_nms(boxes, scores, labels, box_nms_thresh)\n",
    "        # keep only topk scoring predictions\n",
    "        keep = keep[: box_detections_per_img]\n",
    "        boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
    "\n",
    "        all_boxes.append(boxes)\n",
    "        all_scores.append(scores)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    return all_boxes, all_scores, all_labels\n",
    "\n",
    "boxes, scores, labels = postprocess_detections(class_logits, box_regression, proposals, images.image_sizes)\n",
    "boxes, scores, labels = boxes[0], scores[0], labels[0] # we only have one image\n",
    "detections = [{'boxes': boxes, 'scores': scores, 'labels': labels}]\n",
    "detections = transform.postprocess(detections, images.image_sizes, [(image.shape[-2], image.shape[-1])])\n",
    "boxes, scores, labels = detections[0]['boxes'], detections[0]['scores'], detections[0]['labels']\n",
    "labels = [_COCO_CATEGORIES[label.item()] + f': {score:.3f}' for label, score in zip(labels, scores)]\n",
    "vis = draw_bounding_boxes(image, boxes, labels=labels)\n",
    "display(to_pil_image(vis))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
